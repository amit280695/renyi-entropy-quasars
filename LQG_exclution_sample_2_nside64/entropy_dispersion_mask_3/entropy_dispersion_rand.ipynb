{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b738288-df01-4268-853c-16ad00f0d0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import healpy as hp\n",
    "import os\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "from tqdm import tqdm\n",
    "from statistics import mean, stdev\n",
    "\n",
    "# Output directories\n",
    "output_dir = \"Randomized_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "file_inter = \"inter_data2\"\n",
    "os.makedirs(file_inter, exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "nside = 64\n",
    "nf = 100         # Number of bootstrap random realizations\n",
    "n_jobs = 15     # Number of parallel jobs\n",
    "\n",
    "# -----------------------------\n",
    "# Function to generate one randomized file\n",
    "# -----------------------------\n",
    "def randomize_sample(f, r, id_pix):\n",
    "    random.seed((f + 1))  # Ensure reproducibility per bootstrap index\n",
    "\n",
    "    num_points = len(r)\n",
    "    valid_theta = []\n",
    "    valid_phi = []\n",
    "\n",
    "    while len(valid_theta) < num_points:\n",
    "        cos_theta = random.uniform(-1.0, 1.0)\n",
    "        phi = random.uniform(0, 2 * np.pi)\n",
    "        theta = np.arccos(cos_theta)\n",
    "        pix = hp.ang2pix(nside, theta, phi)\n",
    "\n",
    "        if pix in id_pix:\n",
    "            valid_theta.append(theta)\n",
    "            valid_phi.append(phi)\n",
    "\n",
    "    # Save randomized data\n",
    "    v_theta = np.array(valid_theta)\n",
    "    v_phi = np.array(valid_phi)\n",
    "    df_out = pd.DataFrame({'r': r, 'th': v_theta, 'ph': v_phi})\n",
    "    out_file = os.path.join(output_dir, f's2_rand{f+1}.dat')\n",
    "    df_out.to_csv(out_file, sep=\"\\t\", header=None, index=False)\n",
    "\n",
    "    return f\"Saved: {out_file}\"\n",
    "\n",
    "# -----------------------------\n",
    "# Load data for sample 2 \n",
    "# Load original sample data\n",
    "sample_file = f'../Data_exc_lqg_s2/m3_sample2_ex_LQG.dat'\n",
    "df_sample = pd.read_csv(sample_file, sep=\"\\t\", header=None, names=['r', 'th', 'ph'])\n",
    "r = df_sample['r'].to_numpy()\n",
    "\n",
    "# Load valid pixel IDs\n",
    "pix_file = f'../Data_exc_lqg_s2/m3_non_zero_pix_id_s2_ex_LQG.dat'\n",
    "df_pix = pd.read_csv(pix_file, sep=\"\\t\", header=None, names=['id_list'])\n",
    "id_pix = df_pix['id_list'].to_numpy()\n",
    "\n",
    "# -----------------------------\n",
    "# Run randomizations in parallel with tqdm\n",
    "# -----------------------------\n",
    "\n",
    "with parallel_backend(\"loky\", n_jobs=n_jobs):\n",
    "    results = list(\n",
    "        Parallel()(delayed(randomize_sample)(f, r, id_pix) for f in range(nf)))\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Compute RÃ©nyi entropies for q = 1 to 5\n",
    "# -----------------------------------------------------------\n",
    "def compute_renyi_entropy(input_file, output_file, rmin, rmax, Nside=nside, nbin=30):\n",
    "    Npix = hp.nside2npix(Nside)\n",
    "\n",
    "    df = pd.read_csv(input_file, sep=\"\\t\", header=None)\n",
    "    df.columns = ['r', 'th', 'ph']\n",
    "    r1 = df['r'].to_numpy()\n",
    "    th1 = df['th'].to_numpy()\n",
    "    ph1 = df['ph'].to_numpy()\n",
    "\n",
    "    dr = (rmax - rmin) / nbin\n",
    "\n",
    "    m = np.zeros((nbin, Npix), dtype=float)\n",
    "    Neff = np.zeros(nbin, dtype=int)\n",
    "\n",
    "    for rr, tt, pp in zip(r1, th1, ph1):\n",
    "        for j in range(nbin):\n",
    "            if rmin <= rr <= (rmin + (j + 1) * dr):\n",
    "                px = hp.ang2pix(Nside, tt, pp)\n",
    "                m[j][px] += 1\n",
    "                Neff[j] += 1\n",
    "\n",
    "    p = np.zeros((nbin, Npix), dtype=float)\n",
    "    for i in range(nbin):\n",
    "        for px in range(Npix):\n",
    "            if Neff[i] > 0:\n",
    "                p[i][px] = m[i][px] / Neff[i]\n",
    "\n",
    "    H = np.zeros((nbin, 5), dtype=float)\n",
    "    a = np.zeros((nbin, 5), dtype=float)\n",
    "\n",
    "    for k in range(5):\n",
    "        h = np.zeros(nbin, dtype=float)\n",
    "        q = k + 1\n",
    "        if q == 1:\n",
    "            for i in range(nbin):\n",
    "                for px in range(Npix):\n",
    "                    if p[i][px] > 0:\n",
    "                        H[i][k] -= p[i][px] * np.log10(p[i][px])\n",
    "        else:\n",
    "            for i in range(nbin):\n",
    "                for px in range(Npix):\n",
    "                    h[i] += p[i][px] ** q\n",
    "                if h[i] > 0:\n",
    "                    H[i][k] = np.log10(h[i]) / (1 - q)\n",
    "\n",
    "        for i in range(nbin):\n",
    "            a[i][k] = H[i][k]\n",
    "\n",
    "    R = np.zeros(nbin)\n",
    "    for b in range(nbin):\n",
    "        R[b] = rmin + (b + 1) * dr\n",
    "\n",
    "    df1 = pd.DataFrame(data=a)\n",
    "    df2 = pd.DataFrame(data=R)\n",
    "    df3 = pd.concat([df2, df1], axis=1, join='inner')\n",
    "    df3.to_csv(output_file, sep='\\t', header=False, index=False)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Apply entropy computation for sample 2\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "f_in = '../Data_exc_lqg_s2/m3_sample2_ex_LQG.dat'\n",
    "df = pd.read_csv(f_in, sep=\"\\t\", header=None)\n",
    "df.columns = ['r', 'th', 'ph']\n",
    "R_max = df['r'].max()\n",
    "R_min = df['r'].min()\n",
    "\n",
    "# Prepare list of jobs for parallel processing\n",
    "tasks = []\n",
    "for l in range(nf):\n",
    "    f_samp = 'Randomized_data/s2_rand' + str(l + 1) + '.dat'\n",
    "    f_out = 'inter_data2/anis_s2_rand' + str(l + 1) + '.dat'\n",
    "    tasks.append((f_samp, f_out, R_min, R_max))\n",
    "\n",
    "# Execute in parallel with proper tqdm integration\n",
    "\n",
    "with parallel_backend(\"loky\", n_jobs=n_jobs):\n",
    "    results = list(\n",
    "        Parallel()(\n",
    "            delayed(compute_renyi_entropy)(f_samp, f_out, rmin, rmax)\n",
    "            for f_samp, f_out, rmin, rmax in tasks))\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "\n",
    "from statistics import mean, stdev\n",
    "\n",
    "nbin = 30\n",
    "criteria = np.zeros((nf, nbin)) \n",
    "mean_crit = np.zeros(nbin) \n",
    "d_crit = np.zeros(nbin) \n",
    "\n",
    "for f in range(nf):\n",
    "    file = 'inter_data2/anis_s2' + '_rand' + str(f+1) + '.dat'       \n",
    "    RR = np.loadtxt(file)[:, 0]\n",
    "    a1 = np.loadtxt(file)[:, 1]\n",
    "    a2 = np.loadtxt(file)[:, 2]\n",
    "    a3 = np.loadtxt(file)[:, 3]\n",
    "    a4 = np.loadtxt(file)[:, 4]\n",
    "    a5 = np.loadtxt(file)[:, 5]\n",
    "    \n",
    "    a_mean = np.zeros(len(RR))\n",
    "    stab_cri = np.zeros(len(RR))\n",
    "    \n",
    "    for i in range(len(RR)):\n",
    "        a_mean[i] = (a1[i] + a2[i] + a3[i] + a4[i] + a5[i]) / 5.0\n",
    "        stab_cri[i] = np.sqrt((1 / 5) * ((a1[i] - a_mean[i]) ** 2 + (a2[i] - a_mean[i]) ** 2 +\n",
    "                                        (a3[i] - a_mean[i]) ** 2 + (a4[i] - a_mean[i]) ** 2 +\n",
    "                                        (a5[i] - a_mean[i]) ** 2))\n",
    "    \n",
    "    criteria[f] = stab_cri / a_mean\n",
    "\n",
    "for i in range(nbin):\n",
    "    list_crit = [criteria[j][i] for j in range(nf)]\n",
    "    mean_crit[i] = mean(list_crit)        \n",
    "    d_crit[i] = stdev(list_crit)\n",
    "\n",
    "dict1 = {'r': RR, 'crit': mean_crit, 'sd': d_crit} \n",
    "fd = pd.DataFrame(dict1)\n",
    "f_name1 = 'inter_data2/sample_rand_2' + '_criteria.csv' \n",
    "fd.to_csv(f_name1, index=False)\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "#remove unwanted files\n",
    "for l in range(nf):\n",
    "    file_name = 'inter_data2/anis_s2' + '_rand' + str (l+1) + '.dat'\n",
    "    os.remove(file_name)\n",
    "        \n",
    "import shutil\n",
    "shutil.rmtree('Randomized_data')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
