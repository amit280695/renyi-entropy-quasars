{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea93a660-c157-4e70-97fe-591bf1fe8fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import healpy as hp\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from statistics import mean, stdev\n",
    "import os\n",
    "\n",
    "# Create directory for intermediate output\n",
    "file_inter = \"inter_data1\"\n",
    "os.makedirs(file_inter, exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "nside = 64      # Healpix resolution\n",
    "nf = 100       # Number of bootstrap samples\n",
    "nbin = 30      # Number of radial bins\n",
    "use_cores = 15\n",
    "\n",
    "# Compute Rényi entropy for a given dataset\n",
    "def compute_renyi_entropy(input_file, output_file, rmin, rmax, Nside=nside, nbin=30):\n",
    "    Npix = hp.nside2npix(Nside)\n",
    "    df = pd.read_csv(input_file, sep=\"\\t\", header=None)\n",
    "    df.columns = ['r', 'th', 'ph']\n",
    "    r1, th1, ph1 = df['r'].to_numpy(), df['th'].to_numpy(), df['ph'].to_numpy()\n",
    "\n",
    "    dr = (rmax - rmin) / nbin\n",
    "    m = np.zeros((nbin, Npix), dtype=float)\n",
    "    Neff = np.zeros(nbin, dtype=int)\n",
    "\n",
    "    # Fill Healpix map\n",
    "    for rr, tt, pp in zip(r1, th1, ph1):\n",
    "        for j in range(nbin):\n",
    "            if rmin <= rr <= (rmin + (j+1) * dr):\n",
    "                px = hp.ang2pix(Nside, tt, pp)\n",
    "                m[j][px] += 1\n",
    "                Neff[j] += 1\n",
    "\n",
    "    # Normalize to probability\n",
    "    p = np.zeros((nbin, Npix), dtype=float)\n",
    "    for i in range(nbin):\n",
    "        if Neff[i] > 0:\n",
    "            p[i] = m[i] / Neff[i]\n",
    "\n",
    "    # Compute Rényi entropies for q=1 to 5\n",
    "    H = np.zeros((nbin, 5), dtype=float)\n",
    "    for k in range(5):\n",
    "        q = k + 1\n",
    "        for i in range(nbin):\n",
    "            if q == 1:\n",
    "                H[i][k] = -np.sum(p[i][p[i] > 0] * np.log10(p[i][p[i] > 0]))\n",
    "            else:\n",
    "                h_q = np.sum(p[i] ** q)\n",
    "                if h_q > 0:\n",
    "                    H[i][k] = np.log10(h_q) / (1 - q)\n",
    "\n",
    "    R = rmin + (np.arange(1, nbin + 1) * dr)\n",
    "    df_out = pd.concat([pd.DataFrame(R), pd.DataFrame(H)], axis=1)\n",
    "    df_out.to_csv(output_file, sep='\\t', header=False, index=False)\n",
    "\n",
    "# Helper for bootstrapping\n",
    "def process_sample(l, df_base, R_min, R_max, file_inter):\n",
    "    fd = df_base.sample(frac=0.8, replace=True, random_state=l)\n",
    "    f_samp = f\"{file_inter}/sample2_{l+1}.dat\"\n",
    "    f_out2 = f\"{file_inter}/anis_s2_{l+1}.dat\"\n",
    "    fd.to_csv(f_samp, sep=\"\\t\", header=None, index=False)\n",
    "    compute_renyi_entropy(f_samp, f_out2, R_min, R_max)\n",
    "\n",
    "# Read input sample\n",
    "f_in = '../Data_exc_lqg_s2/m4_sample2_ex_LQG.dat'\n",
    "df = pd.read_csv(f_in, sep=\"\\t\", header=None)\n",
    "df.columns = ['r', 'th', 'ph']\n",
    "\n",
    "# Compute global r min/max\n",
    "R_min = df['r'].min()\n",
    "R_max = df['r'].max()\n",
    "\n",
    "# Original entropy computation\n",
    "f_out1 = f\"{file_inter}/renyi_anis_s2.dat\"\n",
    "compute_renyi_entropy(f_in, f_out1, R_min, R_max)\n",
    "\n",
    "# Bootstrap entropy (parallelized)\n",
    "from joblib import parallel_backend\n",
    "\n",
    "# Bootstrap entropy (parallelized with specified backend)\n",
    "with parallel_backend('loky', n_jobs=use_cores):\n",
    "    Parallel()(\n",
    "        delayed(process_sample)(l, df, R_min, R_max, file_inter)\n",
    "        for l in range(nf))\n",
    "########################################################################################################\n",
    "\n",
    "from statistics import mean, stdev\n",
    "nbin = 30\n",
    "\n",
    "# ---- Compute normalized entropy dispersion and save ----\n",
    "file = f'inter_data1/renyi_anis_s2.dat'\n",
    "RR = np.loadtxt(file)[:, 0]\n",
    "s1 = np.loadtxt(file)[:, 1]\n",
    "s2 = np.loadtxt(file)[:, 2]\n",
    "s3 = np.loadtxt(file)[:, 3]\n",
    "s4 = np.loadtxt(file)[:, 4]\n",
    "s5 = np.loadtxt(file)[:, 5]\n",
    "\n",
    "stab_cri = np.zeros(len(RR))\n",
    "s_mean = np.zeros(len(RR))\n",
    "for i in range(len(RR)):\n",
    "    s_mean[i] = (s1[i] + s2[i] + s3[i] + s4[i] + s5[i]) / 5.0\n",
    "    stab_cri[i] = np.sqrt((1 / 5) * ((s1[i] - s_mean[i]) ** 2 + (s2[i] - s_mean[i]) ** 2 +\n",
    "                                     (s3[i] - s_mean[i]) ** 2 + (s4[i] - s_mean[i]) ** 2 +\n",
    "                                     (s5[i] - s_mean[i]) ** 2))\n",
    "frac_cri = stab_cri / s_mean\n",
    "dict1 = {'r': RR, 'crit': frac_cri}\n",
    "fd = pd.DataFrame(dict1)\n",
    "file_out = f'inter_data1/sample2_crit.csv'\n",
    "fd.to_csv(file_out, index=False)\n",
    "\n",
    "# ---- Compute error on entropy criteria over bootstrap samples ----\n",
    "file_cri = f'inter_data1/sample2_crit.csv'\n",
    "df_cri = pd.read_csv(file_cri)\n",
    "crit_samp = df_cri['crit'].to_numpy()\n",
    "\n",
    "criteria = np.zeros((nf, nbin))\n",
    "d_crit = np.zeros(nbin)\n",
    "\n",
    "for f in range(nf):\n",
    "    file = f'inter_data1/anis_s2_{f+1}.dat'\n",
    "    RR = np.loadtxt(file)[:, 0]\n",
    "    a1 = np.loadtxt(file)[:, 1]\n",
    "    a2 = np.loadtxt(file)[:, 2]\n",
    "    a3 = np.loadtxt(file)[:, 3]\n",
    "    a4 = np.loadtxt(file)[:, 4]\n",
    "    a5 = np.loadtxt(file)[:, 5]\n",
    "    a_mean = np.zeros(len(RR))\n",
    "    stab_cri = np.zeros(len(RR))\n",
    "\n",
    "    for i in range(len(RR)):\n",
    "        a_mean[i] = (a1[i] + a2[i] + a3[i] + a4[i] + a5[i]) / 5.0\n",
    "        stab_cri[i] = np.sqrt((1 / 5) * ((a1[i] - a_mean[i]) ** 2 + (a2[i] - a_mean[i]) ** 2 +\n",
    "                                        (a3[i] - a_mean[i]) ** 2 + (a4[i] - a_mean[i]) ** 2 +\n",
    "                                        (a5[i] - a_mean[i]) ** 2))\n",
    "    criteria[f] = stab_cri / a_mean\n",
    "\n",
    "for i in range(nbin):\n",
    "    list_crit = [criteria[j][i] for j in range(nf)]\n",
    "    d_crit[i] = stdev(list_crit)\n",
    "\n",
    "dict1 = {'r': RR, 'crit': crit_samp, 'sd': d_crit}\n",
    "fd_out = pd.DataFrame(dict1)\n",
    "f_name1 = f'inter_data1/sample_2_criteria_err.csv'\n",
    "fd_out.to_csv(f_name1, index=False)\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "f_out1 = 'inter_data1/renyi_anis_s2.dat'\n",
    "os.remove(f_out1)\n",
    "f_out2 = 'inter_data1/sample2_crit.csv'\n",
    "os.remove(f_out2)\n",
    "for l in range(nf):\n",
    "    file_name = 'inter_data1/anis_s2' + '_' + str (l+1) + '.dat'\n",
    "    os.remove(file_name)\n",
    "    f_samp = 'inter_data1/sample2'+ '_' + str(l+1) + '.dat' \n",
    "    os.remove(f_samp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
